# 프로젝트 개요 및 소개
[![Typing SVG](https://readme-typing-svg.demolab.com?font=Dongle&size=60&pause=1000&color=F249F7&background=FFFFFF8D&repeat=false&random=false&width=800&lines=+%EC%9D%8C%EC%84%B1+%EC%9D%B8%EC%8B%9D%EB%B0%8F+%EC%9D%8C%EC%84%B1+%EB%8C%80%EB%8B%B5+AI%2C+%EB%A7%90%ED%95%98%EB%8A%94+GPT)](https://git.io/typing-svg)
   
<br>   
   
# 프로젝트 문제 정의
1. 다양한 플랫폼 및 장치 호환성의 필요성: 음성 인식 및 음성 대답 AI가 다양한 소프트웨어 및 하드웨어 플랫폼에서도 원활하게 동작할 수 있는 유연성이 요구됨.

2. 자연스러운 대화 기능의 부재로 인한 사용자 경험 저하: 대부분의 음성 인식 시스템은 사용자와의 자연스러운 대화를 제공하지 못해 사용자 경험이 제한됨.

3. 높은 성능 요구의 필요성: 실시간 대화 시스템에서는 뛰어난 성능이 요구되는데, 이를 충족시키는 음성 AI 시스템의 중요성이 부각됨.
   
<br>   
   
# 프로젝트 목표

1. 다양한 플랫폼 및 장치 호환성 제공: 다양한 소프트웨어 및 하드웨어 플랫폼에서 호환되는 음성 인식 및 음성 대답 시스템 구축.
    
2. 자연스러운 대화 기능 제공: 사용자와의 자연스러운 대화를 위한 음성 AI 시스템 개발.
    
3. 고성능 음성 처리: 실시간 대화에 필요한 높은 성능을 제공하는 음성 AI 시스템 구현.
    
4. 프라이버시 및 보안 강화: 사용자 음성 데이터의 프라이버시와 보안을 보장하는 메커니즘 구현.
    
5. 다국어 및 다양한 사투리 지원: 다양한 언어 및 지역의 사투리를 인식하고 처리할 수 있는 음성 AI 시스템 개발.
   
<br>   
   
# 시장 조사
음성 인식 및 대화형 AI 시장은 현재 급격한 성장을 경험하고 있습니다. 
인공 지능 기술의 발전과 함께 스마트 홈 장치, 자동차 인포테인먼트 시스템, 의료 및 교육 분야에서의 적용 등 다양한 영역에서 활용이 확대되고 있습니다. 
또한, 사용자 경험의 향상과 효율성 증대를 위해 기업들은 음성 AI 기술에 대한 투자를 늘리고 있습니다. 
이러한 시장 동향을 고려하여 우리 프로젝트는 시장의 수요를 충족시키고 미래 시장에서 경쟁력을 확보할 수 있는 기술을 개발하는 데 주력할 것입니다.
   
<br>   
   
# 시스템 디자인 구상도
![Untitled Diagram drawio](https://github.com/kccistc/intel-04/assets/165994180/50373d35-aadc-4579-9411-e2de5274a67a)
   
<br>   
   
# 시스템 기술 구성도
- #### 10초동안 마이크 입력을 받아서 이를 .wav파일로 변환하는 코드
- #### 음성 -> 텍스트로 변환하는 모델
- #### 받은 음성에 대한 적절한 대답을 하는 LLM 모델
- #### 텍스트 -> 음성으로 변환하여 출력하는 모델
   
<br>   
   
# 개발 진행
   
<br>   
   
## 1. 실시간 음성 녹음
- ### 마이크 입력을 .wav파일로 변환하는 과정을 추가
#### 원래 speech_recognition 모델 코드 세트들의 실행 인자 값으로 -i <장치>를 할려 했으나 wav2vec 모델 세트만 정상적으로 동작하였다. 
#### 해당 speech_recognition_wav2vec 모델은 오직 장치 입력이 아닌 .wav파일만을 처리해서 해당 마이크 입력을 변환하는 과정을 추가하였다.

<br>   
   
## 2. 음성 -> 텍스트로 변환하는 과정
- #### 음성이 변환된 해당 .wav 파일을 텍스트로 변환하도록 speech_recognition_wav2vec 모델 세트 코드를 활용하여 해당 .wav파일을 input.txt파일로 출력하였다.
   
<br>   
   
## 3. GPT로 입력받은 변환하는 과정
- #### 출력된 음성 입력인 inpuit.txt의 내용을 받아서 GPT-2로 입력으로 넣어 output.txt로 내용을 출력하였다.
   
<br>   
   
## 4. 텍스트 -> 음성으로 변환하는 과정
- #### 출력된 LLM모델의 예상 답변인 output.txt를 읽어서 다시 text_to_speech 모델 세트 코드를 활용해서 audio.wav로 출력하였다.
   
<br>   
   
## 5. 과정 병합
- #### 각각의 과정을 os 라이브러리를 활용해서 기존의 각각의 코드를 실행하는 명령어 형식으로 연결하였다.
- #### 기존의 1번과 2번 과정에서 프롬프트 형식의 출력을 파일로 출력될 수 있도록 코드 및 옵션을 추가로 넣었다. 
   
<br>   

# 팀원역활     
<br>   

염재영: 음성 -> 텍스트 변환 및 텍스트-> 음성 변환 모델 검토및 확인, 3가지 모델 병합 작업

김기범: GPT 모델 검토 및 확인, 3가지 모델 병합 작업
     
<br>   

# 시연 및 결과
     
<br>   

# 고찰
- ## 시스템 추후 보완점
- 인식 언어에 대한 한계점으로, 영어 이외의 다른 언어도 사용가능하도록 확장을 할 수 있을 듯 합니다.
- 음성 인식(음성 -> 텍스트)하는 과정의 정확도에 대한 문제로, 음성 인식 모델을 검토하여 다른 모델로 변경 할 수 있을듯 합니다. 
     
<br>   

- ## 팀원 느낀점

김기범 : open model 에서 다른 여러 Speech Recognition 모델들이 있는데 시간 상 성능 비교를 통해 더 나은 모델을 만들지 못한것이 아쉽고, 또 여러 모델들을 병합 하는 과정에 있어서 조금 어려움이 있었습니다. 그리고, 팀프로젝트에 앞서서 미리 관련 모델들을 시험해 보고 확인할 수 있어서 좋았습니다.
     
<br>   

염재영 : openvino의 모델만을 활용해서 음성 인식 AI를 만든것이 신기하고 추후 메인 프로젝트에 도움이 될 기반 시스템을 개발할 수 있는 기회가 된 것 같아서 뿌듯하다.
