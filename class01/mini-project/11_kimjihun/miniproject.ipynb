{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247199f-2cdd-403d-87df-1b2444f755c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### STEP1 컘으로 video 촬영 후 저장하여 pose video로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9a84f-b336-4bec-a2a4-88b9529a82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 필요 lib 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0043ce1a-d522-47ad-b265-c56df656d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import openvino as ov\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fb388-3f29-4215-a343-c680c092e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pose estimation 사용을 위해 필요한 설정들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88982276-1820-4999-a1f8-a4a1ddc6c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = Path(\"model\")\n",
    "\n",
    "# The name of the model from Open Model Zoo.\n",
    "model_name = \"human-pose-estimation-0001\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8).\n",
    "precision = \"FP16-INT8\"\n",
    "\n",
    "model_path = base_model_dir / \"intel\" / model_name / precision / f\"{model_name}.xml\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    model_url_dir = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.1/models_bin/3/{model_name}/{precision}/\"\n",
    "    utils.download_file(model_url_dir + model_name + \".xml\", model_path.name, model_path.parent)\n",
    "    utils.download_file(\n",
    "        model_url_dir + model_name + \".bin\",\n",
    "        model_path.with_suffix(\".bin\").name,\n",
    "        model_path.parent,\n",
    "    )\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device\n",
    "\n",
    "# Initialize OpenVINO Runtime\n",
    "core = ov.Core()\n",
    "# Read the network from a file.\n",
    "model = core.read_model(model_path)\n",
    "# Let the AUTO device decide where to load the model (you can use CPU, GPU as well).\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value, config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "\n",
    "# Get the input and output names of nodes.\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layers = compiled_model.outputs\n",
    "\n",
    "# Get the input size.\n",
    "height, width = list(input_layer.shape)[2:]\n",
    "\n",
    "input_layer.any_name, [o.any_name for o in output_layers]\n",
    "\n",
    "# code from https://github.com/openvinotoolkit/open_model_zoo/blob/9296a3712069e688fe64ea02367466122c8e8a3b/demos/common/python/models/open_pose.py#L135\n",
    "class OpenPoseDecoder:\n",
    "    BODY_PARTS_KPT_IDS = (\n",
    "        (1, 2),\n",
    "        (1, 5),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (1, 8),\n",
    "        (8, 9),\n",
    "        (9, 10),\n",
    "        (1, 11),\n",
    "        (11, 12),\n",
    "        (12, 13),\n",
    "        (1, 0),\n",
    "        (0, 14),\n",
    "        (14, 16),\n",
    "        (0, 15),\n",
    "        (15, 17),\n",
    "        (2, 16),\n",
    "        (5, 17),\n",
    "    )\n",
    "    BODY_PARTS_PAF_IDS = (\n",
    "        12,\n",
    "        20,\n",
    "        14,\n",
    "        16,\n",
    "        22,\n",
    "        24,\n",
    "        0,\n",
    "        2,\n",
    "        4,\n",
    "        6,\n",
    "        8,\n",
    "        10,\n",
    "        28,\n",
    "        30,\n",
    "        34,\n",
    "        32,\n",
    "        36,\n",
    "        18,\n",
    "        26,\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_joints=18,\n",
    "        skeleton=BODY_PARTS_KPT_IDS,\n",
    "        paf_indices=BODY_PARTS_PAF_IDS,\n",
    "        max_points=100,\n",
    "        score_threshold=0.1,\n",
    "        min_paf_alignment_score=0.05,\n",
    "        delta=0.5,\n",
    "    ):\n",
    "        self.num_joints = num_joints\n",
    "        self.skeleton = skeleton\n",
    "        self.paf_indices = paf_indices\n",
    "        self.max_points = max_points\n",
    "        self.score_threshold = score_threshold\n",
    "        self.min_paf_alignment_score = min_paf_alignment_score\n",
    "        self.delta = delta\n",
    "\n",
    "        self.points_per_limb = 10\n",
    "        self.grid = np.arange(self.points_per_limb, dtype=np.float32).reshape(1, -1, 1)\n",
    "\n",
    "    def __call__(self, heatmaps, nms_heatmaps, pafs):\n",
    "        batch_size, _, h, w = heatmaps.shape\n",
    "        assert batch_size == 1, \"Batch size of 1 only supported\"\n",
    "\n",
    "        keypoints = self.extract_points(heatmaps, nms_heatmaps)\n",
    "        pafs = np.transpose(pafs, (0, 2, 3, 1))\n",
    "\n",
    "        if self.delta > 0:\n",
    "            for kpts in keypoints:\n",
    "                kpts[:, :2] += self.delta\n",
    "                np.clip(kpts[:, 0], 0, w - 1, out=kpts[:, 0])\n",
    "                np.clip(kpts[:, 1], 0, h - 1, out=kpts[:, 1])\n",
    "\n",
    "        pose_entries, keypoints = self.group_keypoints(keypoints, pafs, pose_entry_size=self.num_joints + 2)\n",
    "        poses, scores = self.convert_to_coco_format(pose_entries, keypoints)\n",
    "        if len(poses) > 0:\n",
    "            poses = np.asarray(poses, dtype=np.float32)\n",
    "            poses = poses.reshape((poses.shape[0], -1, 3))\n",
    "        else:\n",
    "            poses = np.empty((0, 17, 3), dtype=np.float32)\n",
    "            scores = np.empty(0, dtype=np.float32)\n",
    "\n",
    "        return poses, scores\n",
    "\n",
    "    def extract_points(self, heatmaps, nms_heatmaps):\n",
    "        batch_size, channels_num, h, w = heatmaps.shape\n",
    "        assert batch_size == 1, \"Batch size of 1 only supported\"\n",
    "        assert channels_num >= self.num_joints\n",
    "\n",
    "        xs, ys, scores = self.top_k(nms_heatmaps)\n",
    "        masks = scores > self.score_threshold\n",
    "        all_keypoints = []\n",
    "        keypoint_id = 0\n",
    "        for k in range(self.num_joints):\n",
    "            # Filter low-score points.\n",
    "            mask = masks[0, k]\n",
    "            x = xs[0, k][mask].ravel()\n",
    "            y = ys[0, k][mask].ravel()\n",
    "            score = scores[0, k][mask].ravel()\n",
    "            n = len(x)\n",
    "            if n == 0:\n",
    "                all_keypoints.append(np.empty((0, 4), dtype=np.float32))\n",
    "                continue\n",
    "            # Apply quarter offset to improve localization accuracy.\n",
    "            x, y = self.refine(heatmaps[0, k], x, y)\n",
    "            np.clip(x, 0, w - 1, out=x)\n",
    "            np.clip(y, 0, h - 1, out=y)\n",
    "            # Pack resulting points.\n",
    "            keypoints = np.empty((n, 4), dtype=np.float32)\n",
    "            keypoints[:, 0] = x\n",
    "            keypoints[:, 1] = y\n",
    "            keypoints[:, 2] = score\n",
    "            keypoints[:, 3] = np.arange(keypoint_id, keypoint_id + n)\n",
    "            keypoint_id += n\n",
    "            all_keypoints.append(keypoints)\n",
    "        return all_keypoints\n",
    "\n",
    "    def top_k(self, heatmaps):\n",
    "        N, K, _, W = heatmaps.shape\n",
    "        heatmaps = heatmaps.reshape(N, K, -1)\n",
    "        # Get positions with top scores.\n",
    "        ind = heatmaps.argpartition(-self.max_points, axis=2)[:, :, -self.max_points :]\n",
    "        scores = np.take_along_axis(heatmaps, ind, axis=2)\n",
    "        # Keep top scores sorted.\n",
    "        subind = np.argsort(-scores, axis=2)\n",
    "        ind = np.take_along_axis(ind, subind, axis=2)\n",
    "        scores = np.take_along_axis(scores, subind, axis=2)\n",
    "        y, x = np.divmod(ind, W)\n",
    "        return x, y, scores\n",
    "\n",
    "    @staticmethod\n",
    "    def refine(heatmap, x, y):\n",
    "        h, w = heatmap.shape[-2:]\n",
    "        valid = np.logical_and(np.logical_and(x > 0, x < w - 1), np.logical_and(y > 0, y < h - 1))\n",
    "        xx = x[valid]\n",
    "        yy = y[valid]\n",
    "        dx = np.sign(heatmap[yy, xx + 1] - heatmap[yy, xx - 1], dtype=np.float32) * 0.25\n",
    "        dy = np.sign(heatmap[yy + 1, xx] - heatmap[yy - 1, xx], dtype=np.float32) * 0.25\n",
    "        x = x.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        x[valid] += dx\n",
    "        y[valid] += dy\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def is_disjoint(pose_a, pose_b):\n",
    "        pose_a = pose_a[:-2]\n",
    "        pose_b = pose_b[:-2]\n",
    "        return np.all(np.logical_or.reduce((pose_a == pose_b, pose_a < 0, pose_b < 0)))\n",
    "\n",
    "    def update_poses(\n",
    "        self,\n",
    "        kpt_a_id,\n",
    "        kpt_b_id,\n",
    "        all_keypoints,\n",
    "        connections,\n",
    "        pose_entries,\n",
    "        pose_entry_size,\n",
    "    ):\n",
    "        for connection in connections:\n",
    "            pose_a_idx = -1\n",
    "            pose_b_idx = -1\n",
    "            for j, pose in enumerate(pose_entries):\n",
    "                if pose[kpt_a_id] == connection[0]:\n",
    "                    pose_a_idx = j\n",
    "                if pose[kpt_b_id] == connection[1]:\n",
    "                    pose_b_idx = j\n",
    "            if pose_a_idx < 0 and pose_b_idx < 0:\n",
    "                # Create new pose entry.\n",
    "                pose_entry = np.full(pose_entry_size, -1, dtype=np.float32)\n",
    "                pose_entry[kpt_a_id] = connection[0]\n",
    "                pose_entry[kpt_b_id] = connection[1]\n",
    "                pose_entry[-1] = 2\n",
    "                pose_entry[-2] = np.sum(all_keypoints[connection[0:2], 2]) + connection[2]\n",
    "                pose_entries.append(pose_entry)\n",
    "            elif pose_a_idx >= 0 and pose_b_idx >= 0 and pose_a_idx != pose_b_idx:\n",
    "                # Merge two poses are disjoint merge them, otherwise ignore connection.\n",
    "                pose_a = pose_entries[pose_a_idx]\n",
    "                pose_b = pose_entries[pose_b_idx]\n",
    "                if self.is_disjoint(pose_a, pose_b):\n",
    "                    pose_a += pose_b\n",
    "                    pose_a[:-2] += 1\n",
    "                    pose_a[-2] += connection[2]\n",
    "                    del pose_entries[pose_b_idx]\n",
    "            elif pose_a_idx >= 0 and pose_b_idx >= 0:\n",
    "                # Adjust score of a pose.\n",
    "                pose_entries[pose_a_idx][-2] += connection[2]\n",
    "            elif pose_a_idx >= 0:\n",
    "                # Add a new limb into pose.\n",
    "                pose = pose_entries[pose_a_idx]\n",
    "                if pose[kpt_b_id] < 0:\n",
    "                    pose[-2] += all_keypoints[connection[1], 2]\n",
    "                pose[kpt_b_id] = connection[1]\n",
    "                pose[-2] += connection[2]\n",
    "                pose[-1] += 1\n",
    "            elif pose_b_idx >= 0:\n",
    "                # Add a new limb into pose.\n",
    "                pose = pose_entries[pose_b_idx]\n",
    "                if pose[kpt_a_id] < 0:\n",
    "                    pose[-2] += all_keypoints[connection[0], 2]\n",
    "                pose[kpt_a_id] = connection[0]\n",
    "                pose[-2] += connection[2]\n",
    "                pose[-1] += 1\n",
    "        return pose_entries\n",
    "\n",
    "    @staticmethod\n",
    "    def connections_nms(a_idx, b_idx, affinity_scores):\n",
    "        # From all retrieved connections that share starting/ending keypoints leave only the top-scoring ones.\n",
    "        order = affinity_scores.argsort()[::-1]\n",
    "        affinity_scores = affinity_scores[order]\n",
    "        a_idx = a_idx[order]\n",
    "        b_idx = b_idx[order]\n",
    "        idx = []\n",
    "        has_kpt_a = set()\n",
    "        has_kpt_b = set()\n",
    "        for t, (i, j) in enumerate(zip(a_idx, b_idx)):\n",
    "            if i not in has_kpt_a and j not in has_kpt_b:\n",
    "                idx.append(t)\n",
    "                has_kpt_a.add(i)\n",
    "                has_kpt_b.add(j)\n",
    "        idx = np.asarray(idx, dtype=np.int32)\n",
    "        return a_idx[idx], b_idx[idx], affinity_scores[idx]\n",
    "\n",
    "    def group_keypoints(self, all_keypoints_by_type, pafs, pose_entry_size=20):\n",
    "        all_keypoints = np.concatenate(all_keypoints_by_type, axis=0)\n",
    "        pose_entries = []\n",
    "        # For every limb.\n",
    "        for part_id, paf_channel in enumerate(self.paf_indices):\n",
    "            kpt_a_id, kpt_b_id = self.skeleton[part_id]\n",
    "            kpts_a = all_keypoints_by_type[kpt_a_id]\n",
    "            kpts_b = all_keypoints_by_type[kpt_b_id]\n",
    "            n = len(kpts_a)\n",
    "            m = len(kpts_b)\n",
    "            if n == 0 or m == 0:\n",
    "                continue\n",
    "\n",
    "            # Get vectors between all pairs of keypoints, i.e. candidate limb vectors.\n",
    "            a = kpts_a[:, :2]\n",
    "            a = np.broadcast_to(a[None], (m, n, 2))\n",
    "            b = kpts_b[:, :2]\n",
    "            vec_raw = (b[:, None, :] - a).reshape(-1, 1, 2)\n",
    "\n",
    "            # Sample points along every candidate limb vector.\n",
    "            steps = 1 / (self.points_per_limb - 1) * vec_raw\n",
    "            points = steps * self.grid + a.reshape(-1, 1, 2)\n",
    "            points = points.round().astype(dtype=np.int32)\n",
    "            x = points[..., 0].ravel()\n",
    "            y = points[..., 1].ravel()\n",
    "\n",
    "            # Compute affinity score between candidate limb vectors and part affinity field.\n",
    "            part_pafs = pafs[0, :, :, paf_channel : paf_channel + 2]\n",
    "            field = part_pafs[y, x].reshape(-1, self.points_per_limb, 2)\n",
    "            vec_norm = np.linalg.norm(vec_raw, ord=2, axis=-1, keepdims=True)\n",
    "            vec = vec_raw / (vec_norm + 1e-6)\n",
    "            affinity_scores = (field * vec).sum(-1).reshape(-1, self.points_per_limb)\n",
    "            valid_affinity_scores = affinity_scores > self.min_paf_alignment_score\n",
    "            valid_num = valid_affinity_scores.sum(1)\n",
    "            affinity_scores = (affinity_scores * valid_affinity_scores).sum(1) / (valid_num + 1e-6)\n",
    "            success_ratio = valid_num / self.points_per_limb\n",
    "\n",
    "            # Get a list of limbs according to the obtained affinity score.\n",
    "            valid_limbs = np.where(np.logical_and(affinity_scores > 0, success_ratio > 0.8))[0]\n",
    "            if len(valid_limbs) == 0:\n",
    "                continue\n",
    "            b_idx, a_idx = np.divmod(valid_limbs, n)\n",
    "            affinity_scores = affinity_scores[valid_limbs]\n",
    "\n",
    "            # Suppress incompatible connections.\n",
    "            a_idx, b_idx, affinity_scores = self.connections_nms(a_idx, b_idx, affinity_scores)\n",
    "            connections = list(\n",
    "                zip(\n",
    "                    kpts_a[a_idx, 3].astype(np.int32),\n",
    "                    kpts_b[b_idx, 3].astype(np.int32),\n",
    "                    affinity_scores,\n",
    "                )\n",
    "            )\n",
    "            if len(connections) == 0:\n",
    "                continue\n",
    "\n",
    "            # Update poses with new connections.\n",
    "            pose_entries = self.update_poses(\n",
    "                kpt_a_id,\n",
    "                kpt_b_id,\n",
    "                all_keypoints,\n",
    "                connections,\n",
    "                pose_entries,\n",
    "                pose_entry_size,\n",
    "            )\n",
    "\n",
    "        # Remove poses with not enough points.\n",
    "        pose_entries = np.asarray(pose_entries, dtype=np.float32).reshape(-1, pose_entry_size)\n",
    "        pose_entries = pose_entries[pose_entries[:, -1] >= 3]\n",
    "        return pose_entries, all_keypoints\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_coco_format(pose_entries, all_keypoints):\n",
    "        num_joints = 17\n",
    "        coco_keypoints = []\n",
    "        scores = []\n",
    "        for pose in pose_entries:\n",
    "            if len(pose) == 0:\n",
    "                continue\n",
    "            keypoints = np.zeros(num_joints * 3)\n",
    "            reorder_map = [0, -1, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n",
    "            person_score = pose[-2]\n",
    "            for keypoint_id, target_id in zip(pose[:-2], reorder_map):\n",
    "                if target_id < 0:\n",
    "                    continue\n",
    "                cx, cy, score = 0, 0, 0  # keypoint not found\n",
    "                if keypoint_id != -1:\n",
    "                    cx, cy, score = all_keypoints[int(keypoint_id), 0:3]\n",
    "                keypoints[target_id * 3 + 0] = cx\n",
    "                keypoints[target_id * 3 + 1] = cy\n",
    "                keypoints[target_id * 3 + 2] = score\n",
    "            coco_keypoints.append(keypoints)\n",
    "            scores.append(person_score * max(0, (pose[-1] - 1)))  # -1 for 'neck'\n",
    "        return np.asarray(coco_keypoints), np.asarray(scores)\n",
    "\n",
    "decoder = OpenPoseDecoder()\n",
    "\n",
    "# 2D pooling in numpy (from: https://stackoverflow.com/a/54966908/1624463)\n",
    "def pool2d(A, kernel_size, stride, padding, pool_mode=\"max\"):\n",
    "    \"\"\"\n",
    "    2D Pooling\n",
    "\n",
    "    Parameters:\n",
    "        A: input 2D array\n",
    "        kernel_size: int, the size of the window\n",
    "        stride: int, the stride of the window\n",
    "        padding: int, implicit zero paddings on both sides of the input\n",
    "        pool_mode: string, 'max' or 'avg'\n",
    "    \"\"\"\n",
    "    # Padding\n",
    "    A = np.pad(A, padding, mode=\"constant\")\n",
    "\n",
    "    # Window view of A\n",
    "    output_shape = (\n",
    "        (A.shape[0] - kernel_size) // stride + 1,\n",
    "        (A.shape[1] - kernel_size) // stride + 1,\n",
    "    )\n",
    "    kernel_size = (kernel_size, kernel_size)\n",
    "    A_w = as_strided(\n",
    "        A,\n",
    "        shape=output_shape + kernel_size,\n",
    "        strides=(stride * A.strides[0], stride * A.strides[1]) + A.strides,\n",
    "    )\n",
    "    A_w = A_w.reshape(-1, *kernel_size)\n",
    "\n",
    "    # Return the result of pooling.\n",
    "    if pool_mode == \"max\":\n",
    "        return A_w.max(axis=(1, 2)).reshape(output_shape)\n",
    "    elif pool_mode == \"avg\":\n",
    "        return A_w.mean(axis=(1, 2)).reshape(output_shape)\n",
    "\n",
    "\n",
    "# non maximum suppression\n",
    "def heatmap_nms(heatmaps, pooled_heatmaps):\n",
    "    return heatmaps * (heatmaps == pooled_heatmaps)\n",
    "\n",
    "\n",
    "# Get poses from results.\n",
    "def process_results(img, pafs, heatmaps):\n",
    "    # This processing comes from\n",
    "    # https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/common/python/models/open_pose.py\n",
    "    pooled_heatmaps = np.array([[pool2d(h, kernel_size=3, stride=1, padding=1, pool_mode=\"max\") for h in heatmaps[0]]])\n",
    "    nms_heatmaps = heatmap_nms(heatmaps, pooled_heatmaps)\n",
    "\n",
    "    # Decode poses.\n",
    "    poses, scores = decoder(heatmaps, nms_heatmaps, pafs)\n",
    "    output_shape = list(compiled_model.output(index=0).partial_shape)\n",
    "    output_scale = (\n",
    "        img.shape[1] / output_shape[3].get_length(),\n",
    "        img.shape[0] / output_shape[2].get_length(),\n",
    "    )\n",
    "    # Multiply coordinates by a scaling factor.\n",
    "    poses[:, :, :2] *= output_scale\n",
    "    return poses, scores\n",
    "\n",
    "colors = (\n",
    "    (255, 0, 0),\n",
    "    (255, 0, 255),\n",
    "    (170, 0, 255),\n",
    "    (255, 0, 85),\n",
    "    (255, 0, 170),\n",
    "    (85, 255, 0),\n",
    "    (255, 170, 0),\n",
    "    (0, 255, 0),\n",
    "    (255, 255, 0),\n",
    "    (0, 255, 85),\n",
    "    (170, 255, 0),\n",
    "    (0, 85, 255),\n",
    "    (0, 255, 170),\n",
    "    (0, 0, 255),\n",
    "    (0, 255, 255),\n",
    "    (85, 0, 255),\n",
    "    (0, 170, 255),\n",
    ")\n",
    "\n",
    "default_skeleton = (\n",
    "    (15, 13),\n",
    "    (13, 11),\n",
    "    (16, 14),\n",
    "    (14, 12),\n",
    "    (11, 12),\n",
    "    (5, 11),\n",
    "    (6, 12),\n",
    "    (5, 6),\n",
    "    (5, 7),\n",
    "    (6, 8),\n",
    "    (7, 9),\n",
    "    (8, 10),\n",
    "    (1, 2),\n",
    "    (0, 1),\n",
    "    (0, 2),\n",
    "    (1, 3),\n",
    "    (2, 4),\n",
    "    (3, 5),\n",
    "    (4, 6),\n",
    ")\n",
    "\n",
    "\n",
    "def draw_poses(img, poses, point_score_threshold, skeleton=default_skeleton):\n",
    "    if poses.size == 0:\n",
    "        return img\n",
    "\n",
    "    img_limbs = np.copy(img)\n",
    "    for pose in poses:\n",
    "        points = pose[:, :2].astype(np.int32)\n",
    "        points_scores = pose[:, 2]\n",
    "        # Draw joints.\n",
    "        for i, (p, v) in enumerate(zip(points, points_scores)):\n",
    "            if v > point_score_threshold:\n",
    "                cv2.circle(img, tuple(p), 1, colors[i], 2)\n",
    "        # Draw limbs.\n",
    "        for i, j in skeleton:\n",
    "            if points_scores[i] > point_score_threshold and points_scores[j] > point_score_threshold:\n",
    "                cv2.line(\n",
    "                    img_limbs,\n",
    "                    tuple(points[i]),\n",
    "                    tuple(points[j]),\n",
    "                    color=colors[j],\n",
    "                    thickness=4,\n",
    "                )\n",
    "    cv2.addWeighted(img, 0.4, img_limbs, 0.6, 0, dst=img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ae810-0341-4cf3-8f97-224a4818aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### video 촬영 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f452dd1f-6666-411b-8548-01b5afd8ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # 0은 기본 웹캠을 의미합니다. 다른 카메라를 사용하려면 1, 2 등으로 변경 가능합니다.\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"웹캠을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 동영상 저장을 위한 설정\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # 프레임 너비\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # 프레임 높이\n",
    "\n",
    "# VideoWriter 객체 생성 - mp4 포맷\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 25, (frame_width, frame_height))\n",
    "\n",
    "# 캡처된 화면을 연속적으로 캡처하여 화면에 표시하고 파일에 저장\n",
    "while True:\n",
    "    ret, frame = cap.read()  # 프레임 읽기\n",
    "    if not ret:\n",
    "        print(\"프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "    \n",
    "    out.write(frame)  # 프레임을 동영상 파일에 쓰기\n",
    "    cv2.imshow('Webcam', frame)  # 프레임을 'Webcam' 창에 표시\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # 'q' 키를 누르면 종료\n",
    "        break\n",
    "\n",
    "# 자원 해제q\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bedf0a-a905-4fbb-b3de-d814d16e6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 촬영 한 video를 pose video 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b1325dc-6a54-4067-956e-6ed70739674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "pafs_output_key = compiled_model.output(\"Mconv7_stage2_L1\")\n",
    "heatmaps_output_key = compiled_model.output(\"Mconv7_stage2_L2\")\n",
    "# 저장한 동영상 파일 열기\n",
    "cap = cv2.VideoCapture('output.mp4')\n",
    "\n",
    "# 동영상 파일 열기 확인\n",
    "if not cap.isOpened():\n",
    "    print(\"동영상 파일을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 동영상 파일의 프레임 너비와 높이 가져오기\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# VideoWriter 객체 생성 - 동영상 재생을 위한 창\n",
    "cv2.namedWindow('Saved Video', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Saved Video', frame_width, frame_height)\n",
    "out = cv2.VideoWriter('pose_output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 25, (frame_width, frame_height))\n",
    "\n",
    "# 동영상 재생\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame2 = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "    input_img = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    input_img = input_img.transpose((2, 0, 1))[np.newaxis, ...]\n",
    "    results = compiled_model([input_img])\n",
    "    pafs = results[pafs_output_key]\n",
    "    heatmaps = results[heatmaps_output_key]\n",
    "    poses, scores = process_results(frame2, pafs, heatmaps)\n",
    "    frame2 = draw_poses(frame2, poses, 0.1)\n",
    "    \n",
    "    out.write(frame2)  # 프레임을 동영상 파일에 쓰기\n",
    "    cv2.imshow('Saved Video', frame2)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(40) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc6619-f5fb-4f45-9614-dff359f933e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pose video 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c01212e-a886-48dd-8174-0741678bcbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 저장한 동영상 파일 열기\n",
    "cap = cv2.VideoCapture('pose_output.mp4')\n",
    "\n",
    "# 동영상 파일 열기 확인\n",
    "if not cap.isOpened():\n",
    "    print(\"동영상 파일을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 동영상 파일의 프레임 너비와 높이 가져오기\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# VideoWriter 객체 생성 - 동영상 재생을 위한 창\n",
    "cv2.namedWindow('Saved Video', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Saved Video', frame_width, frame_height)\n",
    "\n",
    "# 동영상 재생\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    " \n",
    "    cv2.imshow('Saved Video', frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(40) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d569d89-219e-4bd1-8a71-61a0f971df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 캐릭터가 움직이는 동영상으로 변환\n",
    "#### input = character , pose video , output = character video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846186c7-91f7-419c-a57e-b926de0e0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "\n",
    "REPO_PATH = Path(\"Moore-AnimateAnyone\")\n",
    "if not REPO_PATH.exists():\n",
    "    !git clone -q \"https://github.com/itrushkin/Moore-AnimateAnyone.git\"\n",
    "%pip install -q \"torch>=2.1\" torchvision einops omegaconf \"diffusers<=0.24\" transformers av accelerate \"openvino>=2024.0\" \"nncf>=2.9.0\" \"gradio>=4.19\" --extra-index-url \"https://download.pytorch.org/whl/cpu\"\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(REPO_PATH.resolve()))\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a71d5c-19ef-4f97-8233-b8a22bac1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SHOULD_CONVERT 변수로 모든 파일이 있는지 확인하고 그렇지 않을시 다시 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909bf8f-79f7-4b7d-ac53-40aaf950b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"models\")\n",
    "VAE_ENCODER_PATH = MODEL_DIR / \"vae_encoder.xml\"\n",
    "VAE_DECODER_PATH = MODEL_DIR / \"vae_decoder.xml\"\n",
    "REFERENCE_UNET_PATH = MODEL_DIR / \"reference_unet.xml\"\n",
    "DENOISING_UNET_PATH = MODEL_DIR / \"denoising_unet.xml\"\n",
    "POSE_GUIDER_PATH = MODEL_DIR / \"pose_guider.xml\"\n",
    "IMAGE_ENCODER_PATH = MODEL_DIR / \"image_encoder.xml\"\n",
    "\n",
    "WIDTH = 112\n",
    "HEIGHT = 128\n",
    "VIDEO_LENGTH = 24\n",
    "\n",
    "SHOULD_CONVERT = not all(\n",
    "    p.exists()\n",
    "    for p in [\n",
    "        VAE_ENCODER_PATH,\n",
    "        VAE_DECODER_PATH,\n",
    "        REFERENCE_UNET_PATH,\n",
    "        DENOISING_UNET_PATH,\n",
    "        POSE_GUIDER_PATH,\n",
    "        IMAGE_ENCODER_PATH,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312a2aa-3fdf-4dbf-bdd1-9a83e5927f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional, Union, List, Callable\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import openvino as ov\n",
    "from torchvision import transforms\n",
    "from einops import repeat\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from transformers import CLIPImageProcessor\n",
    "import torch\n",
    "import gradio as gr\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "\n",
    "from src.pipelines.pipeline_pose2vid_long import Pose2VideoPipeline\n",
    "from src.utils.util import get_fps, read_frames\n",
    "from src.utils.util import save_videos_grid\n",
    "from src.pipelines.context import get_context_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d2402-09e7-418f-b33d-3f84abf1072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "from pathlib import PurePosixPath\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from typing import Dict, Any\n",
    "from diffusers import AutoencoderKL\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "import nncf\n",
    "\n",
    "from src.models.unet_2d_condition import UNet2DConditionModel\n",
    "from src.models.unet_3d import UNet3DConditionModel\n",
    "from src.models.pose_guider import PoseGuider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68b998-c074-43b9-9915-75287a7038ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e51d72-381f-44e3-ac5a-8e7e5009f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "local_dir = Path(\"./pretrained_weights/stable-diffusion-v1-5\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "for hub_file in [\"unet/config.json\", \"unet/diffusion_pytorch_model.bin\"]:\n",
    "    saved_path = local_dir / hub_file\n",
    "    if saved_path.exists():\n",
    "        continue\n",
    "    hf_hub_download(\n",
    "        repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "        subfolder=PurePosixPath(saved_path.parent.name),\n",
    "        filename=PurePosixPath(saved_path.name),\n",
    "        local_dir=local_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e38239-4062-40d9-aeba-36055ca6f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### image encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e466b-19ad-4fd1-b95f-ffc8d3b38432",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "local_dir = Path(\"./pretrained_weights\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "for hub_file in [\"image_encoder/config.json\", \"image_encoder/pytorch_model.bin\"]:\n",
    "    saved_path = local_dir / hub_file\n",
    "    if saved_path.exists():\n",
    "        continue\n",
    "    hf_hub_download(\n",
    "        repo_id=\"lambdalabs/sd-image-variations-diffusers\",\n",
    "        subfolder=PurePosixPath(saved_path.parent.name),\n",
    "        filename=PurePosixPath(saved_path.name),\n",
    "        local_dir=local_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c71942-e4df-48f2-9fc2-d1ae2cf497ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026e0bc-9550-4f7d-bf99-b3da6a4bc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "snapshot_download(\n",
    "    repo_id=\"stabilityai/sd-vae-ft-mse\", local_dir=\"./pretrained_weights/sd-vae-ft-mse\"\n",
    ")\n",
    "snapshot_download(\n",
    "    repo_id=\"patrolli/AnimateAnyone\",\n",
    "    local_dir=\"./pretrained_weights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09b2fb-fa7f-4b29-8293-e5872b3127e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"Moore-AnimateAnyone/configs/prompts/animation.yaml\")\n",
    "infer_config = OmegaConf.load(\"Moore-AnimateAnyone/\" + config.inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7eb4f-e96d-437c-b9f9-0c6759b51460",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "vae = AutoencoderKL.from_pretrained(config.pretrained_vae_path)\n",
    "reference_unet = UNet2DConditionModel.from_pretrained(config.pretrained_base_model_path, subfolder=\"unet\")\n",
    "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "    config.pretrained_base_model_path,\n",
    "    config.motion_module_path,\n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs=infer_config.unet_additional_kwargs,\n",
    ")\n",
    "pose_guider = PoseGuider(320, block_out_channels=(16, 32, 96, 256))\n",
    "image_enc = CLIPVisionModelWithProjection.from_pretrained(config.image_encoder_path)\n",
    "\n",
    "\n",
    "NUM_CHANNELS_LATENTS = denoising_unet.config.in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365f84c-dd93-4ab5-9d7c-7f4650bde009",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "denoising_unet.load_state_dict(\n",
    "    torch.load(config.denoising_unet_path, map_location=\"cpu\"),\n",
    "    strict=False,\n",
    ")\n",
    "reference_unet.load_state_dict(\n",
    "    torch.load(config.reference_unet_path, map_location=\"cpu\"),\n",
    ")\n",
    "pose_guider.load_state_dict(\n",
    "    torch.load(config.pose_guider_path, map_location=\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7af07c-e032-4f68-bc53-f5c2f6c0f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### torch script 청소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04726174-69a9-4b51-8be5-23f63987ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6140f-7785-4686-b613-00cad2c780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "warnings.simplefilter(\"ignore\", torch.jit.TracerWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b94671-e012-42b9-b073-53aa33837e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 모델을 py 형식에서 빠르게 돌리기 위한 최적화 , NNCF 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc0c59-44e8-46fb-8f39-6ab1af961211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "if not VAE_ENCODER_PATH.exists():\n",
    "    class VaeEncoder(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "    \n",
    "        def forward(self, x):\n",
    "            return self.vae.encode(x).latent_dist.mean\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        vae_encoder = ov.convert_model(VaeEncoder(vae), example_input=torch.zeros(1,3,512,448))\n",
    "    vae_encoder = nncf.compress_weights(vae_encoder)\n",
    "    ov.save_model(vae_encoder, VAE_ENCODER_PATH)\n",
    "    del vae_encoder\n",
    "    cleanup_torchscript_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef46d87-c4c5-469e-9d68-07ab5783b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "if not VAE_DECODER_PATH.exists():\n",
    "    class VaeDecoder(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "    \n",
    "        def forward(self, z):\n",
    "            return self.vae.decode(z).sample\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        vae_decoder = ov.convert_model(VaeDecoder(vae), example_input=torch.zeros(1,4,HEIGHT//8,WIDTH//8))\n",
    "    vae_decoder = nncf.compress_weights(vae_decoder)\n",
    "    ov.save_model(vae_decoder, VAE_DECODER_PATH)\n",
    "    del vae_decoder\n",
    "    cleanup_torchscript_cache()\n",
    "del vae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356ef38-5aec-4287-8817-a7e150f3f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "if not REFERENCE_UNET_PATH.exists():\n",
    "    class ReferenceUNetWrapper(torch.nn.Module):\n",
    "        def __init__(self, reference_unet):\n",
    "            super().__init__()\n",
    "            self.reference_unet = reference_unet\n",
    "        \n",
    "        def forward(self, sample, timestep, encoder_hidden_states):\n",
    "            return self.reference_unet(sample, timestep, encoder_hidden_states, return_dict=False)[1]\n",
    "            \n",
    "    sample = torch.zeros(2, 4, HEIGHT // 8, WIDTH // 8)\n",
    "    timestep = torch.tensor(0)\n",
    "    encoder_hidden_states = torch.zeros(2, 1, 768)\n",
    "    reference_unet.eval()\n",
    "    with torch.no_grad():\n",
    "        wrapper =  ReferenceUNetWrapper(reference_unet)\n",
    "        example_input = (sample, timestep, encoder_hidden_states)\n",
    "        ref_features_shapes = {k: v.shape for k, v in wrapper(*example_input).items()}\n",
    "        ov_reference_unet = ov.convert_model(\n",
    "            wrapper,\n",
    "            example_input=example_input,\n",
    "        )\n",
    "    ov_reference_unet = nncf.compress_weights(ov_reference_unet)\n",
    "    ov.save_model(ov_reference_unet, REFERENCE_UNET_PATH)\n",
    "    del ov_reference_unet\n",
    "    del wrapper\n",
    "    cleanup_torchscript_cache()\n",
    "del reference_unet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cab20-d3b6-4083-8bcc-92be195bcadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "if not DENOISING_UNET_PATH.exists():\n",
    "    class DenoisingUNetWrapper(torch.nn.Module):\n",
    "        def __init__(self, denoising_unet):\n",
    "            super().__init__()\n",
    "            self.denoising_unet = denoising_unet\n",
    "        \n",
    "        def forward(\n",
    "            self,\n",
    "            sample,\n",
    "            timestep,\n",
    "            encoder_hidden_states,\n",
    "            pose_cond_fea,\n",
    "            ref_features\n",
    "        ):\n",
    "            return self.denoising_unet(\n",
    "                sample,\n",
    "                timestep,\n",
    "                encoder_hidden_states,\n",
    "                ref_features,\n",
    "                pose_cond_fea=pose_cond_fea,\n",
    "                return_dict=False)\n",
    "\n",
    "    example_input = {\n",
    "        \"sample\": torch.zeros(2, 4, VIDEO_LENGTH, HEIGHT // 8, WIDTH // 8),\n",
    "        \"timestep\": torch.tensor(999),\n",
    "        \"encoder_hidden_states\": torch.zeros(2,1,768),\n",
    "        \"pose_cond_fea\": torch.zeros(2, 320, VIDEO_LENGTH, HEIGHT // 8, WIDTH // 8),\n",
    "        \"ref_features\": {k: torch.zeros(shape) for k, shape in ref_features_shapes.items()}\n",
    "    }\n",
    "    \n",
    "    denoising_unet.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_denoising_unet = ov.convert_model(\n",
    "            DenoisingUNetWrapper(denoising_unet),\n",
    "            example_input=tuple(example_input.values())\n",
    "        )\n",
    "    ov_denoising_unet.inputs[0].get_node().set_partial_shape(ov.PartialShape((2, 4, VIDEO_LENGTH, HEIGHT // 8, WIDTH // 8)))\n",
    "    ov_denoising_unet.inputs[2].get_node().set_partial_shape(ov.PartialShape((2, 1, 768)))\n",
    "    ov_denoising_unet.inputs[3].get_node().set_partial_shape(ov.PartialShape((2, 320, VIDEO_LENGTH, HEIGHT // 8, WIDTH // 8)))\n",
    "    for ov_input, shape in zip(ov_denoising_unet.inputs[4:], ref_features_shapes.values()):\n",
    "        ov_input.get_node().set_partial_shape(ov.PartialShape(shape))\n",
    "        ov_input.get_node().set_element_type(ov.Type.f32)\n",
    "    ov_denoising_unet.validate_nodes_and_infer_types()\n",
    "    ov_denoising_unet = nncf.compress_weights(ov_denoising_unet)\n",
    "    ov.save_model(ov_denoising_unet, DENOISING_UNET_PATH)\n",
    "    del ov_denoising_unet\n",
    "    cleanup_torchscript_cache()\n",
    "del denoising_unet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38b88d-8132-4126-bf60-5c076a327d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "if not POSE_GUIDER_PATH.exists():\n",
    "    pose_guider.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_pose_guider = ov.convert_model(pose_guider, example_input=torch.zeros(1, 3, VIDEO_LENGTH, HEIGHT, WIDTH))\n",
    "    ov_pose_guider = nncf.compress_weights(ov_pose_guider)\n",
    "    ov.save_model(ov_pose_guider, POSE_GUIDER_PATH)\n",
    "    del ov_pose_guider\n",
    "    cleanup_torchscript_cache()\n",
    "del pose_guider\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef980ef-583e-4bf0-92b7-9065ce00da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $SHOULD_CONVERT\n",
    "if not IMAGE_ENCODER_PATH.exists():\n",
    "    image_enc.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_image_encoder = ov.convert_model(image_enc, example_input=torch.zeros(1, 3, 224, 224), input=(1, 3, 224, 224))\n",
    "    ov_image_encoder = nncf.compress_weights(ov_image_encoder)\n",
    "    ov.save_model(ov_image_encoder, IMAGE_ENCODER_PATH)\n",
    "    del ov_image_encoder\n",
    "    cleanup_torchscript_cache()\n",
    "del image_enc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428df06c-5399-4ea6-90b8-a35a21f97e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3905a7e-5356-4b02-8e68-7cbad6d7b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc2d37-3de4-40dc-a47b-1f0f8764939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVPose2VideoPipeline(Pose2VideoPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae_encoder_path=VAE_ENCODER_PATH,\n",
    "        vae_decoder_path=VAE_DECODER_PATH,\n",
    "        image_encoder_path=IMAGE_ENCODER_PATH,\n",
    "        reference_unet_path=REFERENCE_UNET_PATH,\n",
    "        denoising_unet_path=DENOISING_UNET_PATH,\n",
    "        pose_guider_path=POSE_GUIDER_PATH,\n",
    "        device=device.value,\n",
    "    ):\n",
    "        self.vae_encoder = core.compile_model(vae_encoder_path, device)\n",
    "        self.vae_decoder = core.compile_model(vae_decoder_path, device)\n",
    "        self.image_encoder = core.compile_model(image_encoder_path, device)\n",
    "        self.reference_unet = core.compile_model(reference_unet_path, device)\n",
    "        self.denoising_unet = core.compile_model(denoising_unet_path, device)\n",
    "        self.pose_guider = core.compile_model(pose_guider_path, device)\n",
    "        self.scheduler = DDIMScheduler(**OmegaConf.to_container(infer_config.noise_scheduler_kwargs))\n",
    "\n",
    "        self.vae_scale_factor = 8\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "        self.ref_image_processor = VaeImageProcessor(do_convert_rgb=True)\n",
    "        self.cond_image_processor = VaeImageProcessor(do_convert_rgb=True, do_normalize=False)\n",
    "\n",
    "    def decode_latents(self, latents):\n",
    "        video_length = latents.shape[2]\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        latents = rearrange(latents, \"b c f h w -> (b f) c h w\")\n",
    "        # video = self.vae.decode(latents).sample\n",
    "        video = []\n",
    "        for frame_idx in tqdm(range(latents.shape[0])):\n",
    "            video.append(torch.from_numpy(self.vae_decoder(latents[frame_idx : frame_idx + 1])[0]))\n",
    "        video = torch.cat(video)\n",
    "        video = rearrange(video, \"(b f) c h w -> b c f h w\", f=video_length)\n",
    "        video = (video / 2 + 0.5).clamp(0, 1)\n",
    "        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n",
    "        video = video.cpu().float().numpy()\n",
    "        return video\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        ref_image,\n",
    "        pose_images,\n",
    "        width,\n",
    "        height,\n",
    "        video_length,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=3.5,\n",
    "        num_images_per_prompt=1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        output_type: Optional[str] = \"tensor\",\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: Optional[int] = 1,\n",
    "        context_schedule=\"uniform\",\n",
    "        context_frames=24,\n",
    "        context_stride=1,\n",
    "        context_overlap=4,\n",
    "        context_batch_size=1,\n",
    "        interpolation_factor=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        batch_size = 1\n",
    "\n",
    "        # Prepare clip image embeds\n",
    "        clip_image = self.clip_image_processor.preprocess(ref_image.resize((224, 224)), return_tensors=\"pt\").pixel_values\n",
    "        clip_image_embeds = self.image_encoder(clip_image)[\"image_embeds\"]\n",
    "        clip_image_embeds = torch.from_numpy(clip_image_embeds)\n",
    "        encoder_hidden_states = clip_image_embeds.unsqueeze(1)\n",
    "        uncond_encoder_hidden_states = torch.zeros_like(encoder_hidden_states)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            encoder_hidden_states = torch.cat([uncond_encoder_hidden_states, encoder_hidden_states], dim=0)\n",
    "\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            4,\n",
    "            width,\n",
    "            height,\n",
    "            video_length,\n",
    "            clip_image_embeds.dtype,\n",
    "            torch.device(\"cpu\"),\n",
    "            generator,\n",
    "        )\n",
    "\n",
    "        # Prepare extra step kwargs.\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # Prepare ref image latents\n",
    "        ref_image_tensor = self.ref_image_processor.preprocess(ref_image, height=height, width=width)  # (bs, c, width, height)\n",
    "        ref_image_latents = self.vae_encoder(ref_image_tensor)[0]\n",
    "        ref_image_latents = ref_image_latents * 0.18215  # (b, 4, h, w)\n",
    "        ref_image_latents = torch.from_numpy(ref_image_latents)\n",
    "\n",
    "        # Prepare a list of pose condition images\n",
    "        pose_cond_tensor_list = []\n",
    "        for pose_image in pose_images:\n",
    "            pose_cond_tensor = self.cond_image_processor.preprocess(pose_image, height=height, width=width)\n",
    "            pose_cond_tensor = pose_cond_tensor.unsqueeze(2)  # (bs, c, 1, h, w)\n",
    "            pose_cond_tensor_list.append(pose_cond_tensor)\n",
    "        pose_cond_tensor = torch.cat(pose_cond_tensor_list, dim=2)  # (bs, c, t, h, w)\n",
    "        pose_fea = self.pose_guider(pose_cond_tensor)[0]\n",
    "        pose_fea = torch.from_numpy(pose_fea)\n",
    "\n",
    "        context_scheduler = get_context_scheduler(context_schedule)\n",
    "\n",
    "        # denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                noise_pred = torch.zeros(\n",
    "                    (\n",
    "                        latents.shape[0] * (2 if do_classifier_free_guidance else 1),\n",
    "                        *latents.shape[1:],\n",
    "                    ),\n",
    "                    device=latents.device,\n",
    "                    dtype=latents.dtype,\n",
    "                )\n",
    "                counter = torch.zeros(\n",
    "                    (1, 1, latents.shape[2], 1, 1),\n",
    "                    device=latents.device,\n",
    "                    dtype=latents.dtype,\n",
    "                )\n",
    "\n",
    "                # 1. Forward reference image\n",
    "                if i == 0:\n",
    "                    ref_features = self.reference_unet(\n",
    "                        (\n",
    "                            ref_image_latents.repeat((2 if do_classifier_free_guidance else 1), 1, 1, 1),\n",
    "                            torch.zeros_like(t),\n",
    "                            # t,\n",
    "                            encoder_hidden_states,\n",
    "                        )\n",
    "                    ).values()\n",
    "\n",
    "                context_queue = list(\n",
    "                    context_scheduler(\n",
    "                        0,\n",
    "                        num_inference_steps,\n",
    "                        latents.shape[2],\n",
    "                        context_frames,\n",
    "                        context_stride,\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "                num_context_batches = math.ceil(len(context_queue) / context_batch_size)\n",
    "\n",
    "                context_queue = list(\n",
    "                    context_scheduler(\n",
    "                        0,\n",
    "                        num_inference_steps,\n",
    "                        latents.shape[2],\n",
    "                        context_frames,\n",
    "                        context_stride,\n",
    "                        context_overlap,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                num_context_batches = math.ceil(len(context_queue) / context_batch_size)\n",
    "                global_context = []\n",
    "                for i in range(num_context_batches):\n",
    "                    global_context.append(context_queue[i * context_batch_size : (i + 1) * context_batch_size])\n",
    "\n",
    "                for context in global_context:\n",
    "                    # 3.1 expand the latents if we are doing classifier free guidance\n",
    "                    latent_model_input = torch.cat([latents[:, :, c] for c in context]).repeat(2 if do_classifier_free_guidance else 1, 1, 1, 1, 1)\n",
    "                    latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "                    b, c, f, h, w = latent_model_input.shape\n",
    "                    latent_pose_input = torch.cat([pose_fea[:, :, c] for c in context]).repeat(2 if do_classifier_free_guidance else 1, 1, 1, 1, 1)\n",
    "\n",
    "                    pred = self.denoising_unet(\n",
    "                        (\n",
    "                            latent_model_input,\n",
    "                            t,\n",
    "                            encoder_hidden_states[:b],\n",
    "                            latent_pose_input,\n",
    "                            *ref_features,\n",
    "                        )\n",
    "                    )[0]\n",
    "\n",
    "                    for j, c in enumerate(context):\n",
    "                        noise_pred[:, :, c] = noise_pred[:, :, c] + pred\n",
    "                        counter[:, :, c] = counter[:, :, c] + 1\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = (noise_pred / counter).chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "        if interpolation_factor > 0:\n",
    "            latents = self.interpolate_latents(latents, interpolation_factor, latents.device)\n",
    "        # Post-processing\n",
    "        images = self.decode_latents(latents)  # (b, c, f, h, w)\n",
    "\n",
    "        # Convert to tensor\n",
    "        if output_type == \"tensor\":\n",
    "            images = torch.from_numpy(images)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916b8eb-b20e-4219-a77c-6fc62548ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OVPose2VideoPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85ad8d-a826-4292-9dc5-df9f24236dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_images = read_frames(\"pose_output.mp4\")\n",
    "src_fps = get_fps(\"pose_output.mp4\")\n",
    "ref_image = Image.open(\"Moore-AnimateAnyone/configs/inference/ref_images/anyone-5.png\").convert(\"RGB\")\n",
    "pose_list = []\n",
    "for pose_image_pil in pose_images[:VIDEO_LENGTH]:\n",
    "    pose_list.append(pose_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01502960-bb35-44fe-acc0-b15ba9a20819",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = pipe(\n",
    "    ref_image,\n",
    "    pose_list,\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    "    video_length=VIDEO_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199b7fe-9f37-4aa8-bde3-7594a1be0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h, new_w = video.shape[-2:]\n",
    "pose_transform = transforms.Compose([transforms.Resize((new_h, new_w)), transforms.ToTensor()])\n",
    "pose_tensor_list = []\n",
    "for pose_image_pil in pose_images[:VIDEO_LENGTH]:\n",
    "    pose_tensor_list.append(pose_transform(pose_image_pil))\n",
    "\n",
    "ref_image_tensor = pose_transform(ref_image)  # (c, h, w)\n",
    "ref_image_tensor = ref_image_tensor.unsqueeze(1).unsqueeze(0)  # (1, c, 1, h, w)\n",
    "ref_image_tensor = repeat(ref_image_tensor, \"b c f h w -> b c (repeat f) h w\", repeat=VIDEO_LENGTH)\n",
    "pose_tensor = torch.stack(pose_tensor_list, dim=0)  # (f, c, h, w)\n",
    "pose_tensor = pose_tensor.transpose(0, 1)\n",
    "pose_tensor = pose_tensor.unsqueeze(0)\n",
    "video = torch.cat([ref_image_tensor, pose_tensor, video], dim=0)\n",
    "\n",
    "save_dir = Path(\"./output\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "time_str = datetime.now().strftime(\"%H%M\")\n",
    "out_path = save_dir / f\"{date_str}T{time_str}.mp4\"\n",
    "save_videos_grid(\n",
    "    video,\n",
    "    str(out_path),\n",
    "    n_rows=3,\n",
    "    fps=src_fps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72813a9-09b3-49c4-88f9-d5e1385d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(out_path, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351bac6-50ca-4829-b164-4e945996ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    img,\n",
    "    pose_vid,\n",
    "    seed,\n",
    "    guidance_scale,\n",
    "    num_inference_steps,\n",
    "    _=gr.Progress(track_tqdm=True),\n",
    "):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    pose_list = read_frames(pose_vid)[:VIDEO_LENGTH]\n",
    "    video = pipe(\n",
    "        img,\n",
    "        pose_list,\n",
    "        width=WIDTH,\n",
    "        height=HEIGHT,\n",
    "        video_length=VIDEO_LENGTH,\n",
    "        generator=generator,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "    )\n",
    "    new_h, new_w = video.shape[-2:]\n",
    "    pose_transform = transforms.Compose([transforms.Resize((new_h, new_w)), transforms.ToTensor()])\n",
    "    pose_tensor_list = []\n",
    "    for pose_image_pil in pose_list:\n",
    "        pose_tensor_list.append(pose_transform(pose_image_pil))\n",
    "\n",
    "    ref_image_tensor = pose_transform(img)  # (c, h, w)\n",
    "    ref_image_tensor = ref_image_tensor.unsqueeze(1).unsqueeze(0)  # (1, c, 1, h, w)\n",
    "    ref_image_tensor = repeat(ref_image_tensor, \"b c f h w -> b c (repeat f) h w\", repeat=VIDEO_LENGTH)\n",
    "    pose_tensor = torch.stack(pose_tensor_list, dim=0)  # (f, c, h, w)\n",
    "    pose_tensor = pose_tensor.transpose(0, 1)\n",
    "    pose_tensor = pose_tensor.unsqueeze(0)\n",
    "    video = torch.cat([ref_image_tensor, pose_tensor, video], dim=0)\n",
    "\n",
    "    save_dir = Path(\"./output/gradio\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    time_str = datetime.now().strftime(\"%H%M\")\n",
    "    out_path = save_dir / f\"{date_str}T{time_str}.mp4\"\n",
    "    save_videos_grid(\n",
    "        video,\n",
    "        str(out_path),\n",
    "        n_rows=3,\n",
    "        fps=12,\n",
    "    )\n",
    "    return out_path\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate,\n",
    "    [\n",
    "        gr.Image(label=\"Reference Image\", type=\"pil\"),\n",
    "        gr.Video(label=\"Pose video\"),\n",
    "        gr.Slider(\n",
    "            label=\"Seed\",\n",
    "            value=42,\n",
    "            minimum=np.iinfo(np.int32).min,\n",
    "            maximum=np.iinfo(np.int32).max,\n",
    "        ),\n",
    "        gr.Slider(label=\"Guidance scale\", value=3.5, minimum=1.1, maximum=10),\n",
    "        gr.Slider(label=\"Number of inference steps\", value=30, minimum=15, maximum=100),\n",
    "    ],\n",
    "    \"video\",\n",
    "    examples=[\n",
    "        [\n",
    "            \"Moore-AnimateAnyone/configs/inference/ref_images/anyone-2.png\",\n",
    "            \"Moore-AnimateAnyone/configs/inference/pose_videos/anyone-video-2_kps.mp4\",\n",
    "        ],\n",
    "        [\n",
    "            \"Moore-AnimateAnyone/configs/inference/ref_images/anyone-10.png\",\n",
    "            \"Moore-AnimateAnyone/configs/inference/pose_videos/anyone-video-1_kps.mp4\",\n",
    "        ],\n",
    "        [\n",
    "            \"Moore-AnimateAnyone/configs/inference/ref_images/anyone-11.png\",\n",
    "            \"Moore-AnimateAnyone/configs/inference/pose_videos/anyone-video-1_kps.mp4\",\n",
    "        ],\n",
    "        [\n",
    "            \"Moore-AnimateAnyone/configs/inference/ref_images/anyone-3.png\",\n",
    "            \"Moore-AnimateAnyone/configs/inference/pose_videos/anyone-video-2_kps.mp4\",\n",
    "        ],\n",
    "        [\n",
    "            \"Moore-AnimateAnyone/configs/inference/ref_images/anyone-5.png\",\n",
    "            \"Moore-AnimateAnyone/configs/inference/pose_videos/anyone-video-2_kps.mp4\",\n",
    "        ],\n",
    "    ],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(debug=True, share=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
